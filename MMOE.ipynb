{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11eab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as D\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score,log_loss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc21bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数配置\n",
    "\n",
    "config = {\n",
    "    \"train_csv\":'train_noseq.csv',\n",
    "    \"valid_csv\":'valid_noseq.csv',\n",
    "    \"test_csv\":'test_noseq.csv',\n",
    "    \"sparse_cols\":['user_id','item_id','item_type','dayofweek','is_workday','city','county',\n",
    "                  'town','village','lbs_city','lbs_district','hardware_platform','hardware_ischarging',\n",
    "                  'os_type','network_type','position'],\n",
    "    \"dense_cols\" : ['item_expo_1d','item_expo_7d','item_expo_14d','item_expo_30d','item_clk_1d',\n",
    "                   'item_clk_7d','item_clk_14d','item_clk_30d','use_duration'],\n",
    "    \"label_col\":['click','scroll'],\n",
    "    \"num_task\":2,\n",
    "    \"debug_mode\" : True,\n",
    "    \"epoch\" : 10,\n",
    "    \"batch_size\" : 512,\n",
    "    \"num_workers\":0,\n",
    "    \"lr\" : 0.001,\n",
    "    \"device\" : 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8535101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self,config,df,enc_dict=None):\n",
    "        self.config = config\n",
    "        self.df = df\n",
    "        self.enc_dict = enc_dict\n",
    "        for idx, col in enumerate(self.config['label_col']):\n",
    "            self.df = self.df.rename(columns={col :f'task{idx + 1}_label'})\n",
    "        self.dense_cols = list(set(self.config['dense_cols']))\n",
    "        self.sparse_cols = list(set(self.config['sparse_cols']))\n",
    "        self.feature_name = self.dense_cols+self.sparse_cols\n",
    "\n",
    "        #数据编码\n",
    "        if self.enc_dict == None:\n",
    "            self.get_enc_dict()\n",
    "        self.enc_data()\n",
    "    def get_enc_dict(self):\n",
    "        #计算enc_dict\n",
    "        self.enc_dict = dict(zip( list(self.dense_cols+self.sparse_cols),[dict() for _ in range(len(self.dense_cols+self.sparse_cols))]))\n",
    "        for f in self.sparse_cols:\n",
    "            self.df[f] = self.df[f].astype('str')\n",
    "            map_dict = dict(zip(self.df[f].unique(), range(self.df[f].nunique())))\n",
    "            self.enc_dict[f] = map_dict\n",
    "            self.enc_dict[f]['vocab_size'] = self.df[f].nunique()+1\n",
    "\n",
    "        for f in self.dense_cols:\n",
    "            self.enc_dict[f]['min'] = self.df[f].min()\n",
    "            self.enc_dict[f]['max'] = self.df[f].max()\n",
    "\n",
    "        return self.enc_dict\n",
    "\n",
    "    def enc_dense_data(self,col):\n",
    "        return (self.df[col] - self.enc_dict[col]['min']) / (self.enc_dict[col]['max'] - self.enc_dict[col]['min'])\n",
    "\n",
    "    def enc_sparse_data(self,col):\n",
    "        return self.df[col].apply(lambda x : self.enc_dict[col].get(x,0))\n",
    "\n",
    "    def enc_data(self):\n",
    "        #使用enc_dict对数据进行编码\n",
    "        self.enc_df = copy.deepcopy(self.df)\n",
    "\n",
    "        for col in self.dense_cols:\n",
    "            self.enc_df[col] = self.enc_dense_data(col)\n",
    "        for col in self.sparse_cols:\n",
    "            self.enc_df[col] = self.enc_sparse_data(col)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = dict()\n",
    "        for col in self.feature_name:\n",
    "            if col in self.dense_cols:\n",
    "                data[col] = torch.Tensor([self.enc_df[col].iloc[index]]).squeeze(-1)\n",
    "            elif col in self.sparse_cols:\n",
    "                data[col] = torch.Tensor([self.enc_df[col].iloc[index]]).long().squeeze(-1)\n",
    "        for idx, col in enumerate(self.config['label_col']):\n",
    "            data[f'task{idx + 1}_label'] = torch.Tensor([self.enc_df[f'task{idx + 1}_label'].iloc[index]]).squeeze(-1)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.enc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb134e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(config['train_csv'])\n",
    "valid_df = pd.read_csv(config['valid_csv'])\n",
    "test_df = pd.read_csv(config['test_csv'])\n",
    "\n",
    "train_dataset = MultiTaskDataset(config,train_df)\n",
    "enc_dict = train_dataset.get_enc_dict()\n",
    "valid_dataset = MultiTaskDataset(config, valid_df,enc_dict=enc_dict)\n",
    "test_dataset = MultiTaskDataset(config, test_df,enc_dict=enc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfc5b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = D.DataLoader(train_dataset,batch_size=config['batch_size'],shuffle=True,num_workers=config['num_workers'])\n",
    "valid_loader = D.DataLoader(valid_dataset,batch_size=config['batch_size'],shuffle=False,num_workers=config['num_workers'])\n",
    "test_loader = D.DataLoader(test_dataset,batch_size=config['batch_size'],shuffle=False,num_workers=config['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c910e254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_clk_7d': tensor(0.8888),\n",
       " 'item_expo_30d': tensor(0.8498),\n",
       " 'item_expo_1d': tensor(0.9155),\n",
       " 'use_duration': tensor(0.0001),\n",
       " 'item_clk_14d': tensor(0.8849),\n",
       " 'item_clk_30d': tensor(0.8725),\n",
       " 'item_expo_7d': tensor(0.8427),\n",
       " 'item_expo_14d': tensor(0.8571),\n",
       " 'item_clk_1d': tensor(0.9414),\n",
       " 'item_id': tensor(0),\n",
       " 'os_type': tensor(0),\n",
       " 'town': tensor(0),\n",
       " 'lbs_district': tensor(0),\n",
       " 'hardware_ischarging': tensor(0),\n",
       " 'network_type': tensor(0),\n",
       " 'position': tensor(0),\n",
       " 'lbs_city': tensor(0),\n",
       " 'county': tensor(0),\n",
       " 'city': tensor(0),\n",
       " 'dayofweek': tensor(0),\n",
       " 'is_workday': tensor(0),\n",
       " 'item_type': tensor(0),\n",
       " 'hardware_platform': tensor(0),\n",
       " 'user_id': tensor(0),\n",
       " 'village': tensor(0),\n",
       " 'task1_label': tensor(0.),\n",
       " 'task2_label': tensor(0.)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd463a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#基本网络模块\n",
    "\n",
    "#通用Emb\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_dict = None,\n",
    "                 embedding_dim = None):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.enc_dict = enc_dict\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_layer = nn.ModuleDict()\n",
    "\n",
    "        self.emb_feature = []\n",
    "\n",
    "        for col in self.enc_dict.keys():\n",
    "            if 'vocab_size' in self.enc_dict[col].keys():\n",
    "                self.emb_feature.append(col)\n",
    "                self.embedding_layer.update({col : nn.Embedding(\n",
    "                    self.enc_dict[col]['vocab_size'],\n",
    "                    self.embedding_dim,\n",
    "                )})\n",
    "\n",
    "    def forward(self, X):\n",
    "        #对所有的sparse特征挨个进行embedding\n",
    "        feature_emb_list = []\n",
    "        for col in self.emb_feature:\n",
    "            inp = X[col].long().view(-1, 1)\n",
    "            feature_emb_list.append(self.embedding_layer[col](inp))\n",
    "        return feature_emb_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5fc2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device(gpu=-1):\n",
    "    if gpu >= 0 and torch.cuda.is_available():\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "        device = torch.device(f\"cuda:{gpu}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "    \n",
    "def set_activation(activation):\n",
    "    if isinstance(activation, str):\n",
    "        if activation.lower() == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        elif activation.lower() == \"sigmoid\":\n",
    "            return nn.Sigmoid()\n",
    "        elif activation.lower() == \"tanh\":\n",
    "            return nn.Tanh()\n",
    "        else:\n",
    "            return getattr(nn, activation)()\n",
    "    else:\n",
    "        return activation\n",
    "    \n",
    "def get_dnn_input_dim(enc_dict,embedding_dim):\n",
    "    num_sparse = 0\n",
    "    num_dense = 0\n",
    "    for col in enc_dict.keys():\n",
    "        if 'min' in enc_dict[col].keys():\n",
    "            num_dense+=1\n",
    "        elif 'vocab_size' in enc_dict[col].keys():\n",
    "            num_sparse+=1\n",
    "    return num_sparse*embedding_dim+num_dense\n",
    "\n",
    "def get_linear_input(enc_dict,data):\n",
    "    res_data = []\n",
    "    for col in enc_dict.keys():\n",
    "        if 'min' in enc_dict[col].keys():\n",
    "            res_data.append(data[col])\n",
    "    res_data = torch.stack(res_data,axis=1)\n",
    "    return res_data\n",
    "\n",
    "def get_feature_num(enc_dict):\n",
    "    num_sparse = 0\n",
    "    num_dense = 0\n",
    "    for col in enc_dict.keys():\n",
    "        if 'min' in enc_dict[col].keys():\n",
    "            num_dense+=1\n",
    "        elif 'vocab_size' in enc_dict[col].keys():\n",
    "            num_sparse+=1\n",
    "    return num_sparse,num_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46622ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMOE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_task=2,\n",
    "                 n_expert=3,\n",
    "                 embedding_dim=40,\n",
    "                 mmoe_hidden_dim=128,\n",
    "                 expert_activation=None,\n",
    "                 hidden_dim=[128, 64],\n",
    "                 dropouts=[0.2, 0.2],\n",
    "                 enc_dict=None):\n",
    "        super(MMOE, self).__init__()\n",
    "        self.enc_dict = enc_dict\n",
    "        self.num_task = num_task\n",
    "        self.n_expert = n_expert\n",
    "        self.mmoe_hidden_dim = mmoe_hidden_dim\n",
    "        self.expert_activation = expert_activation\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropouts = dropouts\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_layer = EmbeddingLayer(enc_dict=self.enc_dict, embedding_dim=self.embedding_dim)\n",
    "\n",
    "        self.num_sparse_fea, self.num_dense_fea = get_feature_num(self.enc_dict)\n",
    "\n",
    "        hidden_size = self.num_sparse_fea * self.embedding_dim + self.num_dense_fea\n",
    "\n",
    "        # experts\n",
    "        self.experts = torch.nn.Parameter(torch.rand(hidden_size, mmoe_hidden_dim, n_expert), requires_grad=True)\n",
    "        # self.experts.data.normal_(0, 1)\n",
    "        self.experts_bias = torch.nn.Parameter(torch.rand(mmoe_hidden_dim, n_expert), requires_grad=True)\n",
    "        # gates\n",
    "        self.gates = [torch.nn.Parameter(torch.rand(hidden_size, n_expert), requires_grad=True) for _ in\n",
    "                      range(num_task)]\n",
    "        for gate in self.gates:\n",
    "            gate.data.normal_(0, 1)\n",
    "        self.gates_bias = [torch.nn.Parameter(torch.rand(n_expert), requires_grad=True) for _ in range(num_task)]\n",
    "\n",
    "        for i in range(self.num_task):\n",
    "            setattr(self, 'task_{}_dnn'.format(i + 1), nn.ModuleList()) #eval()\n",
    "            hid_dim = [mmoe_hidden_dim] + hidden_dim\n",
    "            for j in range(len(hid_dim) - 1):\n",
    "                getattr(self, 'task_{}_dnn'.format(i + 1)).add_module('ctr_hidden_{}'.format(j),\n",
    "                                                                      nn.Linear(hid_dim[j], hid_dim[j + 1]))\n",
    "                getattr(self, 'task_{}_dnn'.format(i + 1)).add_module('ctr_batchnorm_{}'.format(j),\n",
    "                                                                      nn.BatchNorm1d(hid_dim[j + 1]))\n",
    "                getattr(self, 'task_{}_dnn'.format(i + 1)).add_module('ctr_dropout_{}'.format(j),\n",
    "                                                                      nn.Dropout(dropouts[j]))\n",
    "            getattr(self, 'task_{}_dnn'.format(i + 1)).add_module('task_last_layer', nn.Linear(hid_dim[-1], 1))\n",
    "            getattr(self, 'task_{}_dnn'.format(i + 1)).add_module('task_sigmoid', nn.Sigmoid())\n",
    "\n",
    "    def set_device(self, device):\n",
    "        for i in range(self.num_task):\n",
    "            self.gates[i] = self.gates[i].to(device)\n",
    "            self.gates_bias[i] = self.gates_bias[i].to(device)\n",
    "        print(f'Successfully set device:{device}')\n",
    "\n",
    "    def forward(self, data):\n",
    "        feature_embedding = self.embedding_layer(data)\n",
    "        hidden = torch.stack(feature_embedding, 1).flatten(start_dim=1)\n",
    "\n",
    "        dense_fea = get_linear_input(self.enc_dict, data) #hidden = self.num_sparse_fea * self.embedding_dim + self.num_dense_fea\n",
    "\n",
    "        hidden = torch.cat([hidden, dense_fea], axis=-1) #batch,hidden\n",
    "\n",
    "        # mmoe\n",
    "        #矩阵乘法：torch.bmm(),torch.matmul(),torch.mm().....\n",
    "        '''\n",
    "        i:batch,j:hidden\n",
    "        j:hidden,k:mmoe_hidden_size,l:n_expert\n",
    "        '''\n",
    "        experts_out = torch.einsum('ij, jkl -> ikl', hidden, self.experts)  # batch * mmoe_hidden_size * num_experts\n",
    "        experts_out += self.experts_bias\n",
    "        if self.expert_activation is not None:\n",
    "            experts_out = self.expert_activation(experts_out)\n",
    "\n",
    "                \n",
    "        gates_out = list()\n",
    "        for idx, gate in enumerate(self.gates):\n",
    "            '''\n",
    "            a:batch,b:hidden\n",
    "            b:hidden,c:n_expert\n",
    "            '''\n",
    "            gate_out = torch.einsum('ab, bc -> ac', hidden, gate)  # batch * num_experts\n",
    "            if self.gates_bias:\n",
    "                gate_out += self.gates_bias[idx]\n",
    "            gate_out = nn.Softmax(dim=-1)(gate_out)\n",
    "            gates_out.append(gate_out)\n",
    "\n",
    "        outs = list()\n",
    "        for gate_output in gates_out:\n",
    "            '''\n",
    "            experts_out: batch,mmoe_hidden_size,num_experts\n",
    "            gate_output: batch,num_experts -> batch,mmoe_hidden_size,num_experts\n",
    "            '''\n",
    "            expanded_gate_output = torch.unsqueeze(gate_output, 1)  # batch * 1 * num_experts\n",
    "            weighted_expert_output = experts_out * expanded_gate_output.expand_as(\n",
    "                experts_out)  # batch * mmoe_hidden_size * num_experts\n",
    "            outs.append(torch.sum(weighted_expert_output, 2))  # batch * mmoe_hidden_size\n",
    "\n",
    "        # task tower\n",
    "        output_dict = dict()\n",
    "        task_outputs = list()\n",
    "        for i in range(self.num_task):\n",
    "            x = outs[i]\n",
    "            for mod in getattr(self, 'task_{}_dnn'.format(i + 1)):\n",
    "                x = mod(x)\n",
    "            task_outputs.append(x)\n",
    "            output_dict[f'task{i + 1}_pred'] = x\n",
    "        # get loss\n",
    "        loss = self.loss(task_outputs, data)\n",
    "        output_dict['loss'] = loss\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def loss(self, task_outputs, data, weight=None):\n",
    "        if weight == None:\n",
    "            weight = np.ones(self.num_task) / self.num_task\n",
    "        loss = 0\n",
    "        for i in range(len(task_outputs)):\n",
    "            loss += weight[i] * nn.functional.binary_cross_entropy(task_outputs[i].squeeze(-1)+1e-6,\n",
    "                                                                   data[f'task{i + 1}_label'])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e532d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, device, metric_list=['roc_auc_score','log_loss'], num_task =1):\n",
    "    model.train()\n",
    "    if num_task == 1:\n",
    "        pred_list = []\n",
    "        label_list = []\n",
    "        pbar = tqdm(train_loader)\n",
    "        for data in pbar:\n",
    "\n",
    "            for key in data.keys():\n",
    "                data[key] = data[key].to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            pred = output['pred']\n",
    "            loss = output['loss']\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "            pred_list.extend(pred.squeeze(-1).cpu().detach().numpy())\n",
    "            label_list.extend(data['label'].squeeze(-1).cpu().detach().numpy())\n",
    "            pbar.set_description(\"Loss {}\".format(loss))\n",
    "\n",
    "        res_dict = dict()\n",
    "        for metric in metric_list:\n",
    "            if metric =='log_loss':\n",
    "                res_dict[f'train_{metric}'] = log_loss(label_list,pred_list, eps=1e-7)\n",
    "            else:\n",
    "                res_dict[f'train_{metric}'] = eval(metric)(label_list,pred_list)\n",
    "\n",
    "        return\n",
    "    else:\n",
    "        multi_task_pred_list = [[] for _ in range(num_task)]\n",
    "        multi_task_label_list = [[] for _ in range(num_task)]\n",
    "        pbar = tqdm(train_loader)\n",
    "        for data in pbar:\n",
    "\n",
    "            for key in data.keys():\n",
    "                data[key] = data[key].to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = output['loss']\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            for i in range(num_task):\n",
    "                multi_task_pred_list[i].extend(list(output[f'task{i + 1}_pred'].squeeze(-1).cpu().detach().numpy()))\n",
    "                multi_task_label_list[i].extend(list(data[f'task{i + 1}_label'].squeeze(-1).cpu().detach().numpy()))\n",
    "            pbar.set_description(\"Loss {}\".format(loss))\n",
    "\n",
    "        res_dict = dict()\n",
    "        for i in range(num_task):\n",
    "            for metric in metric_list:\n",
    "                if metric == 'log_loss':\n",
    "                    res_dict[f'train_task{i+1}_{metric}'] = log_loss(multi_task_label_list[i], multi_task_pred_list[i], eps=1e-7)\n",
    "                else:\n",
    "                    res_dict[f'train_task{i+1}_{metric}'] = eval(metric)(multi_task_label_list[i], multi_task_pred_list[i])\n",
    "        return res_dict\n",
    "\n",
    "def valid_model(model, valid_loader, device, metric_list=['roc_auc_score','log_loss'],num_task =1):\n",
    "    model.eval()\n",
    "    if num_task == 1:\n",
    "        pred_list = []\n",
    "        label_list = []\n",
    "        for data in tqdm(valid_loader):\n",
    "\n",
    "            for key in data.keys():\n",
    "                data[key] = data[key].to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            pred = output['pred']\n",
    "\n",
    "            pred_list.extend(pred.squeeze(-1).cpu().detach().numpy())\n",
    "            label_list.extend(data['label'].squeeze(-1).cpu().detach().numpy())\n",
    "\n",
    "        res_dict = dict()\n",
    "        for metric in metric_list:\n",
    "            if metric =='log_loss':\n",
    "                res_dict[f'valid_{metric}'] = log_loss(label_list,pred_list, eps=1e-7)\n",
    "            else:\n",
    "                res_dict[f'valid_{metric}'] = eval(metric)(label_list,pred_list)\n",
    "\n",
    "        return res_dict\n",
    "    else:\n",
    "        multi_task_pred_list = [[] for _ in range(num_task)]\n",
    "        multi_task_label_list = [[] for _ in range(num_task)]\n",
    "        for data in valid_loader:\n",
    "\n",
    "            for key in data.keys():\n",
    "                data[key] = data[key].to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            for i in range(num_task):\n",
    "                multi_task_pred_list[i].extend(list(output[f'task{i + 1}_pred'].squeeze(-1).cpu().detach().numpy()))\n",
    "                multi_task_label_list[i].extend(list(data[f'task{i + 1}_label'].squeeze(-1).cpu().detach().numpy()))\n",
    "\n",
    "        res_dict = dict()\n",
    "        for i in range(num_task):\n",
    "            for metric in metric_list:\n",
    "                if metric == 'log_loss':\n",
    "                    res_dict[f'valid_task{i+1}_{metric}'] = log_loss(multi_task_label_list[i], multi_task_pred_list[i], eps=1e-7)\n",
    "                else:\n",
    "                    res_dict[f'valid_task{i+1}_{metric}'] = eval(metric)(multi_task_label_list[i], multi_task_pred_list[i])\n",
    "        return res_dict\n",
    "\n",
    "def test_model(model, test_loader, device, metric_list=['roc_auc_score','log_loss'],num_task =1):\n",
    "    model.eval()\n",
    "    if num_task == 1:\n",
    "        pred_list = []\n",
    "        label_list = []\n",
    "        for data in tqdm(test_loader):\n",
    "\n",
    "            for key in data.keys():\n",
    "                data[key] = data[key].to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            pred = output['pred']\n",
    "\n",
    "            pred_list.extend(pred.squeeze(-1).cpu().detach().numpy())\n",
    "            label_list.extend(data['label'].squeeze(-1).cpu().detach().numpy())\n",
    "\n",
    "        res_dict = dict()\n",
    "        for metric in metric_list:\n",
    "            if metric == 'log_loss':\n",
    "                res_dict[f'test_{metric}'] = log_loss(label_list, pred_list, eps=1e-7)\n",
    "            else:\n",
    "                res_dict[f'test_{metric}'] = eval(metric)(label_list, pred_list)\n",
    "\n",
    "        return res_dict\n",
    "    else:\n",
    "        multi_task_pred_list = [[] for _ in range(num_task)]\n",
    "        multi_task_label_list = [[] for _ in range(num_task)]\n",
    "        for data in test_loader:\n",
    "\n",
    "            for key in data.keys():\n",
    "                data[key] = data[key].to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            for i in range(num_task):\n",
    "                multi_task_pred_list[i].extend(list(output[f'task{i + 1}_pred'].squeeze(-1).cpu().detach().numpy()))\n",
    "                multi_task_label_list[i].extend(list(data[f'task{i + 1}_label'].squeeze(-1).cpu().detach().numpy()))\n",
    "\n",
    "        res_dict = dict()\n",
    "        for i in range(num_task):\n",
    "            for metric in metric_list:\n",
    "                if metric == 'log_loss':\n",
    "                    res_dict[f'test_task{i + 1}_{metric}'] = log_loss(multi_task_label_list[i], multi_task_pred_list[i],\n",
    "                                                                 eps=1e-7)\n",
    "                else:\n",
    "                    res_dict[f'test_task{i + 1}_{metric}'] = eval(metric)(multi_task_label_list[i], multi_task_pred_list[i])\n",
    "        return res_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a24dbf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MMOE(enc_dict=enc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a3b51c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.21911370754241943: 100%|███████████████████████████████████████████| 363/363 [05:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:\n",
      "{'train_task1_roc_auc_score': 0.5690551256967591, 'train_task1_log_loss': 0.3429387164022583, 'train_task2_roc_auc_score': 0.5558025842083217, 'train_task2_log_loss': 0.4047207226744438}\n",
      "Valid Metric:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.22694231569766998: 100%|███████████████████████████████████████████| 363/363 [05:04<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:\n",
      "{'train_task1_roc_auc_score': 0.7201995323064578, 'train_task1_log_loss': 0.18646943025961882, 'train_task2_roc_auc_score': 0.643444685036402, 'train_task2_log_loss': 0.2702671524461413}\n",
      "Valid Metric:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.19703590869903564: 100%|███████████████████████████████████████████| 363/363 [04:53<00:00,  1.24it/s]\n",
      "Loss 0.1906682252883911: 100%|████████████████████████████████████████████| 363/363 [05:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:\n",
      "{'train_task1_roc_auc_score': 0.773343736574327, 'train_task1_log_loss': 0.17231779247282358, 'train_task2_roc_auc_score': 0.686045363670525, 'train_task2_log_loss': 0.25854246215160065}\n",
      "Valid Metric:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.20725512504577637: 100%|███████████████████████████████████████████| 363/363 [05:05<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:\n",
      "{'train_task1_roc_auc_score': 0.7943071665175145, 'train_task1_log_loss': 0.16643851440382257, 'train_task2_roc_auc_score': 0.7080943582420599, 'train_task2_log_loss': 0.25378364326784864}\n",
      "Valid Metric:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.17220842838287354: 100%|███████████████████████████████████████████| 363/363 [05:06<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:\n",
      "{'train_task1_roc_auc_score': 0.8183920108029228, 'train_task1_log_loss': 0.16022485525541283, 'train_task2_roc_auc_score': 0.7387514433894964, 'train_task2_log_loss': 0.24665741245579187}\n",
      "Valid Metric:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.17191338539123535: 100%|███████████████████████████████████████████| 363/363 [04:59<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:\n",
      "{'train_task1_roc_auc_score': 0.8405782576902938, 'train_task1_log_loss': 0.15328210445466148, 'train_task2_roc_auc_score': 0.7694727938695708, 'train_task2_log_loss': 0.23870366522537825}\n",
      "Valid Metric:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.1915300041437149:  31%|█████████████▊                              | 114/363 [01:32<03:31,  1.18it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = set_device(config['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "model = model.to(device)\n",
    "#模型训练流程\n",
    "for i in range(config['epoch']):\n",
    "    #模型训练\n",
    "    train_metirc = train_model(model,train_loader,optimizer=optimizer,device=device,num_task=config['num_task'])\n",
    "    #模型验证\n",
    "    valid_metric = valid_model(model,valid_loader,device,num_task=config['num_task'])\n",
    "\n",
    "    print(\"Train Metric:\")\n",
    "    print(train_metirc)\n",
    "    print(\"Valid Metric:\")\n",
    "#测试模型\n",
    "test_metric = test_model(model,test_loader,device,num_task=config['num_task'])\n",
    "print('Test Metric:')\n",
    "print(test_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "528a96d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_task1_roc_auc_score': 0.7683968055513677,\n",
       " 'test_task1_log_loss': 0.1874662285723263,\n",
       " 'test_task2_roc_auc_score': 0.662262847074074,\n",
       " 'test_task2_log_loss': 0.29286973183338544}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#可能的开源～涉及rank模型以及多任务模型～有兴趣的私聊我\n",
    "'''\n",
    "train_loader,valid_laoder,test_loader, enc_dict = get_dataloader(train_df,valid_df,test_df,config)\n",
    "\n",
    "model = MMOE(enc_dict=enc_dict,**config)\n",
    "\n",
    "trainer = Trainer(model,config)\n",
    "\n",
    "trainer.fit(train_loader,valid_loader,**config)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
